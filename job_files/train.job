#!/bin/bash

#SBATCH --partition=gpu_a100
#SBATCH --gpus=1
#SBATCH --job-name=training
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=18
#SBATCH --time=18:00:00
#SBATCH --output=logs/training_%A.out

module purge
module load 2024
module load Anaconda3/2024.06-1
module load CUDA/12.6.0

# Set CUDA_HOME environment variable
export CUDA_HOME=$CUDA_ROOT
export CUDA_PATH=$CUDA_ROOT

source activate lean-reinforcement

# Source environment variables
set -a
source $HOME/lean_reinforcement/.env
set +a

cd $HOME/lean_reinforcement

echo "Starting training run with distributed environment..."
srun python3 -m src.training.train \
    --data-type novel_premises \
    --num-epochs 10 \
    --num-theorems 100 \
    --num-iterations 50 \
    --max-steps 30 \
    --mcts-type guided_rollout \
    --train-epochs 1 \
    --train-value-head \
    --use-final-reward \
    --save-training-data \
    --save-checkpoints \
    --use-wandb
